"""
ê·œì •Â·ì§€ì¹¨ ê²€ìƒ‰ Tool â€” LangChain @tool ë˜í¼.

regulations(ë²•ë¥  + ê·œì •) ì»¬ë ‰ì…˜ê³¼ guidelines(ì§€ì¹¨) ì»¬ë ‰ì…˜ì—ì„œ
ê´€ë ¨ ê·¼ê±°ë¥¼ ë²¡í„° ê²€ìƒ‰ â†’ Cohere ë¦¬ë­í‚¹í•˜ì—¬ ë°˜í™˜í•œë‹¤.
"""

from __future__ import annotations

from langchain_core.tools import tool

from providers.embed_openai import OpenAIEmbedProvider
from storage.chroma_store import chroma_store
from utils.reranker import rerank_chunks  # ğŸ†• ë¦¬ë­ì»¤ ì¶”ê°€

_LAW_DOC_TYPES = {"ë²•ë ¹", "law"}
_embedder: OpenAIEmbedProvider | None = None


def _get_query_embedding(query: str) -> list[float]:
    global _embedder
    if _embedder is None:
        _embedder = OpenAIEmbedProvider()
    return _embedder.embed([query])[0]


def _parse_query_result(raw: dict) -> list[dict]:
    ids = raw.get("ids", [[]])[0] or []
    documents = raw.get("documents", [[]])[0] or []
    metadatas = raw.get("metadatas", [[]])[0] or []
    distances = raw.get("distances", [[]])[0] or []

    chunks: list[dict] = []
    for idx in range(len(ids)):
        distance = distances[idx] if idx < len(distances) else 0.0
        chunks.append({
            "content": documents[idx] if idx < len(documents) else "",
            "metadata": metadatas[idx] if idx < len(metadatas) else {},
            "chroma_id": ids[idx],
            "relevance_score": round(1.0 - distance, 4),
        })
    return chunks


@tool
def search_policy(query: str) -> dict:
    """ê·œì •Â·ì§€ì¹¨ ê²€ìƒ‰: ì£¼ì–´ì§„ ì§ˆì˜ì™€ ê´€ë ¨ëœ ë²•ë¥ , ê·œì •, ì§€ì¹¨ ê·¼ê±°ë¥¼ ë²¡í„°DBì—ì„œ ê²€ìƒ‰í•©ë‹ˆë‹¤.

    Args:
        query: ê²€ìƒ‰í•  ì§ˆì˜ ë¬¸ìì—´ (ì˜ˆ: "ë°©ì†¡ í•œì •íŒë§¤ ê¸´ê¸‰ì„± í‘œí˜„")

    Returns:
        law_chunks â€” ë²•ë¥  ê´€ë ¨ ì²­í¬
        regulation_chunks â€” ê·œì • ê´€ë ¨ ì²­í¬
        guideline_chunks â€” ì§€ì¹¨ ê´€ë ¨ ì²­í¬
    """
    query_embedding = _get_query_embedding(query)

    # 1) regulations ì»¬ë ‰ì…˜ ê²€ìƒ‰ (ê¸°ì¡´ 10 â†’ 20)
    reg_raw = chroma_store.query(
        collection_key="regulations",
        query_embeddings=[query_embedding],
        n_results=20,  # ğŸ†• ë¦¬ë­í‚¹ìš©ìœ¼ë¡œ ë„‰ë„‰í•˜ê²Œ
    )
    reg_chunks = _parse_query_result(reg_raw)

    # 2) ë¦¬ë­í‚¹ ë¨¼ì €, ê·¸ í›„ ë²•ë¥ /ê·œì • ë¶„ë¥˜
    reg_reranked = rerank_chunks(query=query, chunks=reg_chunks, top_n=10)

    law_chunks: list[dict] = []
    regulation_chunks: list[dict] = []
    for chunk in reg_reranked:
        dt = chunk["metadata"].get("doc_type", "")
        if dt in _LAW_DOC_TYPES:
            law_chunks.append(chunk)
        else:
            regulation_chunks.append(chunk)

    # 3) guidelines ì»¬ë ‰ì…˜ ê²€ìƒ‰ (ê¸°ì¡´ 5 â†’ 15)
    guide_raw = chroma_store.query(
        collection_key="guidelines",
        query_embeddings=[query_embedding],
        n_results=15,  # ğŸ†• ë¦¬ë­í‚¹ìš©ìœ¼ë¡œ ë„‰ë„‰í•˜ê²Œ
    )
    guideline_chunks = _parse_query_result(guide_raw)

    # 4) ì§€ì¹¨ë„ ë¦¬ë­í‚¹
    guideline_reranked = rerank_chunks(query=query, chunks=guideline_chunks, top_n=5)

    return {
        "law_chunks": law_chunks,
        "regulation_chunks": regulation_chunks,
        "guideline_chunks": guideline_reranked,
    }


@tool
def fetch_chunk_by_id(chroma_id: str, collection_key: str) -> dict:
    """Chroma IDë¡œ íŠ¹ì • ì²­í¬ ì›ë¬¸ì„ ì¡°íšŒí•©ë‹ˆë‹¤."""
    coll = chroma_store.get_collection(collection_key)
    result = coll.get(ids=[chroma_id])

    ids = result.get("ids", [])
    documents = result.get("documents", [])
    metadatas = result.get("metadatas", [])

    if not ids:
        return {"content": "", "metadata": {}, "chroma_id": chroma_id}

    return {
        "content": documents[0] if documents else "",
        "metadata": metadatas[0] if metadatas else {},
        "chroma_id": ids[0],
    }